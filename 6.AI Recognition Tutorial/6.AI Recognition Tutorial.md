# 6.AI Recognition Tutorial

Note: need to turn off the programme of boot-up, then follow the below
instructions

Open the terminal and input sudo nano /etc/rc.local then press Enter

![](/media/3db288837d9f1fa26f3fefed5db2eaad.png)

Add \# before enabling the scripts, as shown below;

![](/media/fd09271425f350ce513d497bd8b0c079.png)

Press Ctrl + o to save, then press Enter and Ctrl + X to exit editing

Contents

1\. Drive the Camera 2

[1.1 cv2.VideoCapture() function 2](\\l)

[1.2 cap.set() function 3](\\l)

[1.3 cap.isOpened() function 4](\\l)

[1.4 ret,frame = cap.read() function 5](\\l)

[1.5 cv2.waitKey() function 5](\\l)

[1.6 cap.release() and destroyAllWindows() function 5](\\l)

[1.7 Example code for driving a camera 5](\\l)

[1.8 Run the code and test result 6](\\l)

[**2.** Color recognition 7](\\l)

[2.1 HSV color space 7](\\l)

[2.2 Color space conversion 8](\\l)

[2.3 Sample code for color recognition 9](\\l)

[2.4 Run the codes and test result 11](\\l)

[3. Color Recognition and Tracking 11](\\l)

[3.1 Code to Identify colors and track 12](\\l)

[3.2 Run the code and test result 16](\\l)

[4. QR Code Identification 16](\\l)

[4.1 QR Code Generator 17](\\l)

[4.2 Test Code 17](\\l)

[4.3 Run the code and test result 19](\\l)

[5. Face Recognition 19](\\l)

[5.1 Face Detection Code 22](\\l)

[5.2 Run the code and test result 24](\\l)

[6. TensorFlow Object Recognition 24](\\l)

[6.1 Test Code 26](\\l)

[6.2 Run the code and test result 28](\\l)

## 1.Drive the Camera 
    
    Use “opencv” to drive the camera. The “API” function provided is
    very powerful and flexible, now let’s introduce it as follow.
    
    1.  ### cv2.VideoCapture() function
        
        cap = cv2.VideoCapture(0)
        
        0 is the parameter of this function, indicating Raspberry Pi
        “video0”.
        
        （Note：The current camera can be viewed with the command “ls
        /dev/”.)
        
        ![](/media/333d2a0156310284b49ff3386fa454c0.png)
    
    2.  ### cap.set() function
        
        cap.set(CV\_CAP\_PROP\_FRAME\_WIDTH, 500) \#set the frame width
        of the vide to 500
        
        Camera configuration methods:
        
        CV\_CAP\_PROP\_POS\_MSEC =0, //obtain an image at certain time
        
        CV\_CAP\_PROP\_POS\_FRAMES =1, //Get a frame of an image
        
        CV\_CAP\_PROP\_POS\_AVI\_RATIO =2, //Get an image of the \*
        position
        
        CV\_CAP\_PROP\_FRAME\_WIDTH =3, //Gets frame width
        
        CV\_CAP\_PROP\_FRAME\_HEIGHT =4, //Gets frame height
        
        CV\_CAP\_PROP\_FPS =5, //Gets the frame rate
        
        CV\_CAP\_PROP\_FOURCC =6, // 4-character Encoder
        
        CV\_CAP\_PROP\_FRAME\_COUNT =7, //Gets the total number of
        frames
        
        CV\_CAP\_PROP\_FORMAT =8, //Video formats
        
        CV\_CAP\_PROP\_MODE =9, //Boolean flags indicating whether
        images should be converted to RGB.
        
        CV\_CAP\_PROP\_BRIGHTNESS =10, //Brightness
        
        CV\_CAP\_PROP\_CONTRAST =11, //Contrast Ratio
        
        CV\_CAP\_PROP\_SATURATION =12, //Saturation
        
        CV\_CAP\_PROP\_HUE =13, //Hue
        
        CV\_CAP\_PROP\_GAIN =14, //Gain
        
        CV\_CAP\_PROP\_EXPOSURE =15, //Exposure
        
        CV\_CAP\_PROP\_CONVERT\_RGB =16, //Stereo camera correction mark
        
        CV\_CAP\_PROP\_WHITE\_BALANCE\_BLUE\_U =17, //White balance
    
    3.  ### cap.isOpened() function
        
        Used to determine if the camera is turned on successfully. If it
        is successful, return “ture”, otherwise “false”.
    
    4.  ### ret,frame = cap.read() function

“cap.read()”reads the video frame by frame.“ret,frame” are the two
return values to get the “cap.read()”method.“ret” is a boolean value .
If the frame is right, returns True, and if the file is not read to the
end, its return value is False

The frame is the image of each frame, which is a three-dimensional
matrix.

### cv2.waitKey() function

The parameter is 1, which means delay 1ms to switch to the next frame.

The parameter is 0, which indicates that only the current frame image is
displayed(equivalent to video pause).

### 1.6 cap.release() and destroyAllWindows() function

“cap.release()” releases the video and uses“destroyAllWindows()” to
close all image windows.

### 1.7 Example code for driving a camera

<table>
<tbody>
<tr class="odd">
<td><p>import cv2</p>
<p>def useCamera():</p>
<p># Obtaining a Camera</p>
<p>capture = cv2.VideoCapture(0)</p>
<p>capture.set(3, 480) # Gets the video frame width</p>
<p>capture.set(10, 55) # Screen brightness</p>
<p>while capture.isOpened():</p>
<p># Open the camera and read the image</p>
<p>flag, image = capture.read()</p>
<p>cv2.imshow("image", image)</p>
<p>k = cv2.waitKey(1)</p>
<p>if k == ord('s'):</p>
<p>cv2.imwrite("test.jpg", image)</p>
<p>elif k == ord("q"):</p>
<p>break</p>
<p># release the camera</p>
<p>capture.release()</p>
<p># Close all Windows</p>
<p>cv2.destroyAllWindows()</p>
<p>if __name__ == '__main__':</p>
<p>useCamera()</p></td>
</tr>
</tbody>
</table>

### 1.8 Run the code and test result

cd /home/pi/RaspberryPi-Car/Ai\_recognition

sudo python3 Ai1\_Camera\_driver.py

After running the code, a window will appear on the desktop to display
the image captured by the camera.

![](/media/28df3999808c657f255bc8ad1c882d62.png)

## 2.Color recognition
    
    1.  ### HSV color space 
        
        we often use the RGB color space, mixing red, green and blue,
        and can synthesize various colors. But in the machine, it is not
        easy to recognizes the proportion of each color. The HSV color
        space is composed of H (Hue), S (Saturation), and V (Value).
        According to the “H”, a certain color can be basically
        determined, and some value can be determined as greater than a
        threshold by the saturation and brightness information. This
        makes it easier for the machine to recognize colors.
        
        ![](/media/df854717015567bc444a2d8cd6445403.png)
    
    2.  ### Color space conversion
        
        Opencv incorporates multiple color space conversion functions.
        We usually fucntion use BGR \>\> Gray and BGR \>\> HSV. Note
        that Gray can’t convert with HSV. Function is as follows;
        
        cv2.cvtColor(image, flag)
        
        When BGR \>\> Gray，the parameter flag can be written as
        cv2.COLOR\_BGR2GRAY
        
        When BGR \>\> HSV，the parameter flag: cv2.COLOR\_BGR2HSV
        
        Pay attention to the value range of HSV color space in “opencv”：
        
        H\[0,179\] S\[0,255\] V\[0,255\]
        
        Range of common colors：
        
        ![](/media/d4d05416c54005d43556bd5b50a91c30.jpeg)
    
    3.  ### Sample code for color recognition
        
        First set to color range white), and the rest to black, then
        replace the white parts with the original images.

<table>
<tbody>
<tr class="odd">
<td><p>import cv2</p>
<p>import numpy as np</p>
<p>import time</p>
<p>cap = cv2.VideoCapture(0)</p>
<p>cap.set(3, 480)</p>
<p>cap.set(4, 380)</p>
<p>cap.set(5, 120) #Set the frame rate</p>
<p>cap.set(cv2.CAP_PROP_FOURCC, cv2.VideoWriter.fourcc('M', 'J', 'P', 'G'))</p>
<p>cap.set(cv2.CAP_PROP_BRIGHTNESS, 60) #Set the brightness -64 - 64 0.0</p>
<p>cap.set(cv2.CAP_PROP_CONTRAST, 40) #Set contrast -64 - 64 2.0</p>
<p># image.set(cv2.CAP_PROP_EXPOSURE, 156) #Set exposure 1.0 - 5000 156.0</p>
<p># The red areas</p>
<p>color_lower = np.array([0, 43, 46])</p>
<p>color_upper = np.array([10, 255, 255])</p>
<p># #Green range</p>
<p>#color_lower = np.array([35, 43, 46])</p>
<p>#color_upper = np.array([77, 255, 255])</p>
<p># #Blue range</p>
<p>#color_lower=np.array([100, 43, 46])</p>
<p>#color_upper = np.array([124, 255, 255])</p>
<p># #Yellow range</p>
<p># color_lower = np.array([26, 43, 46])</p>
<p># color_upper = np.array([34, 255, 255])</p>
<p># #Orange range</p>
<p># color_lower = np.array([11, 43, 46])</p>
<p># color_upper = np.array([25, 255, 255])</p>
<p>def Color_Recongnize():</p>
<p>while(1):</p>
<p>ret, frame = cap.read()</p>
<p>cv2.imshow('Capture', frame)</p>
<p># change to hsv model</p>
<p>hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)</p>
<p># get mask</p>
<p># The inRange() function and the upper and lower bounds of the blue</p>
<p># range in HSV model are used to obtain the mask. The blue part of the</p>
<p># original video of the mask will be made white, and the other parts will be black.</p>
<p>mask = cv2.inRange(hsv, color_lower, color_upper)</p>
<p>cv2.imshow('Mask', mask)</p>
<p>#mask_widget.value = bgr8_to_jpeg(mask)</p>
<p># detect blue</p>
<p># When the mask is pressed and operated on the original video frame,</p>
<p># white in the mask will be replaced with the real image:</p>
<p>res = cv2.bitwise_and(frame, frame, mask=mask)</p>
<p>cv2.imshow('Result', res)</p>
<p>if cv2.waitKey(1) &amp; 0xFF == ord('q'):</p>
<p>break</p>
<p>time.sleep(0.01)</p>
<p># Exit when closed</p>
<p>cap.release()</p>
<p>cv2.destroyAllWindows()</p>
<p># main function</p>
<p>Color_Recongnize()</p></td>
</tr>
</tbody>
</table>

4.  ### Run the codes and test result
    
    cd /home/pi/RaspberryPi-Car/Ai\_recognition
    
    sudo python3 Ai2\_Color\_identification.py
    
    You can see that only the red square is identified in the screen
    monitored by the camera. Press keyboard “q” to exit.
    
    ![](/media/72641a46e821d8024db8c3ef88ddb9b3.png)

<!-- end list -->

## 3.Color Recognition and Tracking
    
    We’ve learned the color recognition from the previous lesson. In
    this lesson, we will achieve color recognition and tracking by the
    4DOF pan tilt.
    
    1.  ### Code to Identify colors and track 

<table>
<tbody>
<tr class="odd">
<td><p>import RPi.GPIO as GPIO</p>
<p>import time</p>
<p>import enum</p>
<p>import cv2</p>
<p>import time</p>
<p># Thread function operation library</p>
<p>import threading</p>
<p>import inspect</p>
<p>import ctypes</p>
<p>import numpy as np</p>
<p>import PID</p>
<p># servo pin</p>
<p>ServoPin = 7 #S2</p>
<p>ServoPinB = 6 #S3</p>
<p>#Set GPIO BCM</p>
<p>GPIO.setmode(GPIO.BCM)</p>
<p>#servo output mode</p>
<p>def init():</p>
<p>GPIO.setup(ServoPin, GPIO.OUT)</p>
<p>GPIO.setup(ServoPinB, GPIO.OUT)</p>
<p>#Defines an impulse function used to generate PWM values in analog mode</p>
<p>#The time-base pulse is 20ms,</p>
<p>#and the high level part of the pulse controls 0-180 degrees in 0.5-2.5ms</p>
<p>def servo_pulse(myangleA, myangleB):</p>
<p>pulsewidth = myangleA</p>
<p>GPIO.output(ServoPin, GPIO.HIGH)</p>
<p>time.sleep(pulsewidth/1000000.0)</p>
<p>GPIO.output(ServoPin, GPIO.LOW)</p>
<p>time.sleep(20.0/1000-pulsewidth/1000000.0)</p>
<p>pulsewidthB = 2500-myangleB</p>
<p>GPIO.output(ServoPinB, GPIO.HIGH)</p>
<p>time.sleep(pulsewidthB/1000000.0)</p>
<p>GPIO.output(ServoPinB, GPIO.LOW)</p>
<p>time.sleep(20.0/1000-pulsewidthB/1000000.0)</p>
<p>#According to the steering gear pulse control range of 500-2500USEC:</p>
<p>def Servo_control(angle_1, angle_2):</p>
<p>init()</p>
<p>if angle_1 &lt; 500:</p>
<p>angle_1 = 500</p>
<p>elif angle_1 &gt; 2500:</p>
<p>angle_1 = 2500</p>
<p>if angle_2 &lt; 500:</p>
<p>angle_2 = 500</p>
<p>elif angle_2 &gt; 2500:</p>
<p>angle_2 = 2500</p>
<p>servo_pulse(angle_1, angle_2)</p>
<p># Thread closing function</p>
<p>def _async_raise(tid, exctype):</p>
<p>"""raises the exception, performs cleanup if needed"""</p>
<p>tid = ctypes.c_long(tid)</p>
<p>if not inspect.isclass(exctype):</p>
<p>exctype = type(exctype)</p>
<p>res = ctypes.pythonapi.PyThreadState_SetAsyncExc(tid, ctypes.py_object(exctype))</p>
<p>if res == 0:</p>
<p>raise ValueError("invalid thread id")</p>
<p>elif res != 1:</p>
<p># """if it returns a number greater than one, you're in trouble,</p>
<p># and you should call it again with exc=NULL to revert the effect"""</p>
<p>ctypes.pythonapi.PyThreadState_SetAsyncExc(tid, None)</p>
<p>def stop_thread(thread):</p>
<p>_async_raise(thread.ident, SystemExit)</p>
<p># Enabling the Camera</p>
<p>image = cv2.VideoCapture(0)</p>
<p>image.set(3, 640)</p>
<p>image.set(4, 480)</p>
<p>image.set(5, 120) #Set the frame rate</p>
<p># fourcc = cv2.VideoWriter_fourcc(*"MPEG")</p>
<p>image.set(cv2.CAP_PROP_FOURCC, cv2.VideoWriter.fourcc('M', 'J', 'P', 'G'))</p>
<p>image.set(cv2.CAP_PROP_BRIGHTNESS, 55) #Set the brightness-64 - 64 0.0</p>
<p>image.set(cv2.CAP_PROP_CONTRAST, 20) #Set contrast -64 - 64 2.0</p>
<p>global color_x, color_y, color_radius</p>
<p>color_x = color_y = color_radius = 0</p>
<p>global target_valuex</p>
<p>target_valuex = 1500</p>
<p>global target_valuey</p>
<p>target_valuey = 1500</p>
<p>global g_mode</p>
<p>g_mode = 0</p>
<p>global color_lower</p>
<p>#color_lower = np.array([156, 43, 46])</p>
<p>#color_lower = np.array([0, 43, 46]) # red</p>
<p>color_lower=np.array([100, 43, 46]) # blue</p>
<p>#color_lower = np.array([35, 43, 46]) #green</p>
<p>global color_upperv</p>
<p>#color_upper = np.array([180, 255, 255])</p>
<p>#color_upper = np.array([10, 255, 255]) # red</p>
<p>color_upper = np.array([124, 255, 255]) # blue</p>
<p>#color_upper = np.array([77, 255, 255]) #green</p>
<p>target_valuex = target_valuey = 2048</p>
<p>Servo_control(1500, 1500)</p>
<p>xservo_pid = PID.PositionalPID(0.8, 0.1, 0.3)</p>
<p>yservo_pid = PID.PositionalPID(0.4, 0.1, 0.2)</p>
<p># Camera head movement</p>
<p>def Color_track():</p>
<p>global color_lower, color_upper, g_mode</p>
<p>global target_valuex, target_valuey</p>
<p>t_start = time.time()</p>
<p>fps = 0</p>
<p>times = 0</p>
<p>print("start")</p>
<p>while True:</p>
<p>ret, frame = image.read() # Read the video by frame</p>
<p>frame = cv2.resize(frame, (300, 300)) # Image size</p>
<p>frame_ = cv2.GaussianBlur(frame,(5,5),0) # Gaussian filter</p>
<p>hsv = cv2.cvtColor(frame,cv2.COLOR_BGR2HSV) # HSV</p>
<p>mask = cv2.inRange(hsv,color_lower,color_upper) # Specify color range</p>
<p>mask = cv2.erode(mask,None,iterations=2)</p>
<p>mask = cv2.dilate(mask,None,iterations=2)</p>
<p>mask = cv2.GaussianBlur(mask,(3,3),0)</p>
<p>cnts = cv2.findContours(mask.copy(),cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)[-2]</p>
<p>#if g_mode == 1: # Button switch</p>
<p>if len(cnts) &gt; 0:</p>
<p>cnt = max (cnts, key = cv2.contourArea)</p>
<p>(color_x,color_y),color_radius = cv2.minEnclosingCircle(cnt)</p>
<p>if color_radius &gt; 10:</p>
<p>times = times + 1</p>
<p># Mark detected colors</p>
<p>cv2.circle(frame,(int(color_x),int(color_y)),int(color_radius),(255,0,255),2)</p>
<p># Proportion-Integration-Differentiation</p>
<p>xservo_pid.SystemOutput = color_x</p>
<p>xservo_pid.SetStepSignal(150)</p>
<p>xservo_pid.SetInertiaTime(0.01, 0.1)</p>
<p>target_valuex = int(1500+xservo_pid.SystemOutput)</p>
<p># Input Y direction parameter PID control input</p>
<p>yservo_pid.SystemOutput = color_y</p>
<p>yservo_pid.SetStepSignal(150)</p>
<p>yservo_pid.SetInertiaTime(0.01, 0.1)</p>
<p>target_valuey = int(1500+yservo_pid.SystemOutput)</p>
<p>print(target_valuey)</p>
<p># Turn the head holder to the PID setting position</p>
<p>time.sleep(0.008)</p>
<p>if times == 5 :</p>
<p>times = 0</p>
<p>Servo_control(target_valuex,target_valuey)</p>
<p>fps = fps + 1</p>
<p>mfps = fps / (time.time() - t_start)</p>
<p>cv2.putText(frame, "FPS " + str(int(mfps)), (40,40), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0,255,255), 3)</p>
<p># Real-time return image data for display</p>
<p>cv2.imshow("resuilt",frame) # Display renderings</p>
<p>if cv2.waitKey(10) &amp; 0xFF == ord('q'):</p>
<p>#End the process, release the camera, and execute as needed</p>
<p>#stop_thread(thread1)</p>
<p>image.release()</p>
<p>break</p>
<p>#stop_thread(thread1)</p>
<p>image.release()</p>
<p>cv2.destroyAllWindows()</p>
<p>Color_track()</p>
<p># Start the process</p>
<p>#thread1 = threading.Thread(target=Color_track) # Enter the head control main process</p>
<p>#thread1.setDaemon(True)</p>
<p>#thread1.start()</p></td>
</tr>
</tbody>
</table>

2.  ### Run the code and test result
    
    cd /home/pi/RaspberryPi-Car/Ai\_recognition
    
    sudo python3 Ai3\_color\_follow.py
    
    After running the code, take a blue object and move it slowly before
    the camera monitoring range, then camera head will also slowly move,
    then maintain it in the center of the monitoring range of the camera
    . You can modify the code to identify the color you want.
    
    ![](/media/bb6954cf36a40bd978a9173ce85be62c.png)

<!-- end list -->

## 4.QR Code Identification
    
    QR code is also known as two-dimensional barcode. A two-dimensional
    barcode is a black-and-white graphic that records data symbol
    information.

### 4.1 QR Code Generator

Link：<https://www.the-qrcode-generator.com/>

![](/media/d4eb34169adcadcd4bd8dd6e1c5f7165.png)

### 4.2 Test Code

In China, QR code identification technology is a part of our daily life.

We utilized the pyzbar package. If you don’t use the Raspberry Pi imager
system, you need to install pyzbar

sudo apt-get install libzbar0

sudo pip3 install pyzbar

<table>
<tbody>
<tr class="odd">
<td><p>import enum</p>
<p>import cv2</p>
<p># import the necessary packages</p>
<p>#import simple_barcode_detection</p>
<p>import numpy as np</p>
<p>import pyzbar.pyzbar as pyzbar</p>
<p>from PIL import Image</p>
<p># Define and parse two-dimensional code</p>
<p>def decodeDisplay(image):</p>
<p>barcodes = pyzbar.decode(image)</p>
<p>for barcode in barcodes:</p>
<p># Extract the position of the boundary box of the TWO-DIMENSIONAL code</p>
<p># Draw the bounding box for the bar code in the image</p>
<p>(x, y, w, h) = barcode.rect</p>
<p>cv2.rectangle(image, (x, y), (x + w, y + h), (225, 225, 225), 2)</p>
<p># Extract qr code data as byte objects, so if we want to output images on</p>
<p># To draw it, you have to convert it to a string</p>
<p>barcodeData = barcode.data.decode("utf-8")</p>
<p>barcodeType = barcode.type</p>
<p># Plot the barcode data and barcode type on the image</p>
<p>text = "{} ({})".format(barcodeData, barcodeType)</p>
<p>cv2.putText(image, text, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX,0.5, (225, 225, 225), 2)</p>
<p># Prints barcode data and barcode type to the terminal</p>
<p>print("[INFO] Found {} barcode: {}".format(barcodeType, barcodeData))</p>
<p>return image</p>
<p>def detect():</p>
<p>camera = cv2.VideoCapture(0)</p>
<p>camera.set(3, 320)</p>
<p>camera.set(4, 240)</p>
<p>camera.set(5, 120) #Set the frame rate</p>
<p># fourcc = cv2.VideoWriter_fourcc(*"MPEG")</p>
<p>camera.set(cv2.CAP_PROP_FOURCC, cv2.VideoWriter.fourcc('M', 'J', 'P', 'G'))</p>
<p>camera.set(cv2.CAP_PROP_BRIGHTNESS, 50) #Set the brightness -64 - 64 0.0</p>
<p>camera.set(cv2.CAP_PROP_CONTRAST, 50) #Set contrast -64 - 64 2.0</p>
<p>camera.set(cv2.CAP_PROP_EXPOSURE, 156) #Set exposure 1.0 - 5000 156.0</p>
<p>ret, frame = camera.read()</p>
<p>while True:</p>
<p># Read current frame</p>
<p>ret, frame = camera.read()</p>
<p>#Grayscale image</p>
<p>gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)</p>
<p>im = decodeDisplay(gray)</p>
<p>cv2.waitKey(5)</p>
<p>cv2.imshow("image",im)</p>
<p># If you press Q, the loop will be broken</p>
<p>if cv2.waitKey(10) &amp; 0xFF == ord('q'):</p>
<p>break</p>
<p>camera.release()</p>
<p>cv2.destroyAllWindows()</p>
<p>while 1:</p>
<p>detect()</p></td>
</tr>
</tbody>
</table>

### 4.3 Run the code and test result

cd /home/pi/RaspberryPi-Car/Ai\_recognition

sudo python3 Ai4\_Qr\_code.py

After running the code, put the QR code generated in the surveillance
area of the camera, and the terminal can identify the information of the
QR code

![](/media/4911965c37ab1c6678ddeb41a299e35d.png)

## 5.Face Recognition

Face recognition is now very common in technology products. The basic
task of face recognition is face detection. It means that the face can
be captured before further comparison and recognition can be performed.
We only provide a method for face detection.

The most common method of face detection is to use the "Haar Cascade
Classifier". Target detection using a cascade classifier based on Haar
features is an efficient target detection method proposed by Paul Viola
and Michael Jones in the paper "Rapid Object Detection using a Boosted
Cascade of Simple Features" in 2001. This is basically a machine
learning based approach where a cascade function is trained from a lot
of images both positive and negative. Based on the training it is then
used to detect the objects in the other images. We will use it for face
recognition. Initially, the algorithm requires a large number of
positive images (face images) and negative images (images without faces)
to train the classifier, and then extract features from them. “OpenCV”
has trainers and detectors. If you want to train your own object
classifier, such as cars, airplanes, etc., you can use “OpenCV” to
create one. We are using a pre-trained classifier. In “OpenCV”, static
images and real-time video have similar operations for face detection.
In other words, video face detection just reads each frame of image from
the camera, and then uses static image detection methods for detection.
Classifiers required for face detection:

Face detector (default): haarcascade\_frontalface\_default.xml

Face detector (fast Harr): haarcascade\_frontalface\_alt2.xml

Face detector (side view): haarcascade\_profileface.xml

Eye detector (left eye): haarcascade\_lefteye\_2splits.xml

Eye detector (right eye): haarcascade\_righteye\_2splits.xml

Mouth detector: haarcascade\_mcs\_mouth.xml

Nose detector: haarcascade\_mcs\_nose.xml

Body detector: haarcascade\_fullbody.xml

Face detector (fast LBP): lbpcascade\_frontalface.xml

Only open eyes can be detected: haarcascade\_eye.xml

Only detectable when the subject wears glasses:
haarcascade\_eye\_tree\_eyeglasses.xml

Link:https://github.com/opencv/opencv/tree/master/data

Note：coordinate(X)，coordinate(Y)，width(W)，height(H)

haarcascade\_profileface.xml is the cascade data of Haar. xml is
obtained from the data/haarcascades, We can detect faces with
face\_cascade.detectMultiScale(). Yet we can’t load each image obtained
from the camera to the .detectMultiScale(), on the contrary, we should
convert the image into the gray image instead because the face
recognition needs this color space.

（Note：input haarcascade\_profileface.xml at right place）

Detected API function in the OpenCV:

**detectMultiScale (const Mat& image, vector& objects, double
scaleFactor=1.1，int minNeighbors, int flag，cvSize)**

  - Image: input the gray image

  - Objects: Rectangular box vector group obtains the detected object

  - scaleFactor：scale parameter of each image, the default value is 1.1.

<!-- end list -->

  - minNeighbors: minNeighbors controls the error detection, default
    value is 3, which indicates at least three times overlap detection.

  - minSize: the minimum size of the object

  - maxSize: the maximum size of the object
    
    1.  ### Face Detection Code

<table>
<tbody>
<tr class="odd">
<td><p>import enum</p>
<p>import cv2</p>
<p>#Camera display module</p>
<p>import cv2</p>
<p>import time</p>
<p>import sys</p>
<p>image = cv2.VideoCapture(0)</p>
<p>image.set(3,320)</p>
<p>image.set(4,240)</p>
<p>image.set(cv2.CAP_PROP_BRIGHTNESS, 50) #Set the brightness -64 - 64 0.0</p>
<p>ret, frame = image.read()</p>
<p># Face recognition</p>
<p># body_haar = cv2.CascadeClassifier("haarcascade_upperbody.xml")</p>
<p>face_haar = cv2.CascadeClassifier("haarcascade_profileface.xml")</p>
<p>#face_haar = cv2.CascadeClassifier("haarcascade_fullbody.xml")</p>
<p>#eye_haar = cv2.CascadeClassifier("haarcascade_eye.xml")</p>
<p>#eye_haar = cv2.CascadeClassifier("haarcascade_eye_tree_eyeglasses.xml")</p>
<p>def Camera_display():</p>
<p>while 1:</p>
<p>ret, frame = image.read()</p>
<p># Convert the image to black and white</p>
<p>gray_img = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)</p>
<p># # Detect all pedestrians in the image</p>
<p># bodies = body_haar.detectMultiScale(gray_img, 1.3, 5)</p>
<p># for body_x,body_y,body_w,body_h in bodies:</p>
<p># cv2.rectangle(frame, (body_x, body_y), (body_x+body_w, body_y+body_h), (0,255,0), 2)</p>
<p># detectMultiScale（const Mat&amp; image, vector&amp; objects, double scaleFactor=1.1，int minNeighbors, int flag，cvSize）</p>
<p># 1. image is the gray scale image of the input</p>
<p># 2. Objects is a vector group of rectangular boxes to get the detected object</p>
<p># 3. ScaleFactor is the scale parameter in each image scale, with a default value of 1.1.</p>
<p># The scale_factor parameter determines how much of a jump there is between scans of two Windows of different sizes.</p>
<p># Setting this parameter to large means that the computation is faster, but if the window misses a face of a certain size, the object may be lost.</p>
<p># 4. MinNeighbors parameters for each cascade number of adjacent rectangular should be retained (can't understand this parameter, -_ - | | |), the default value is 3.</p>
<p># MinNeighbors controls false detection, with a default value of 3 indicating at least three overlapping detections before we think a face actually exists.</p>
<p># 5. MinSize indicates the minimum size of the target</p>
<p># 6. MaxSize is the maximum size of the target</p>
<p>faces = face_haar.detectMultiScale(gray_img, 1.1, 3)</p>
<p>for face_x,face_y,face_w,face_h in faces:</p>
<p>cv2.rectangle(frame, (face_x, face_y), (face_x+face_w, face_y+face_h), (0,255,0), 2)</p>
<p>(face_x, face_y, face_w, face_h) = faces[0]</p>
<p>print(faces[0:1])</p>
<p>'''</p>
<p>eyes = eye_haar.detectMultiScale(gray_img, 1.1, 3)</p>
<p>for eye_x,eye_y,eye_w,eye_h in eyes:</p>
<p>cv2.rectangle(frame, (eye_x,eye_y), (eye_x+eye_w, eye_y+eye_h), (255,0,0), 2)</p>
<p># eyes = eye_haar.detectMultiScale(gray_img, 1.3, 5)</p>
<p># for eye_x,eye_y,eye_w,eye_h in eyes:</p>
<p># cv2.rectangle(frame, (eye_x,eye_y), (eye_x+eye_w, eye_y+eye_h), (255,0,0), 2)</p>
<p>'''</p>
<p>cv2.imshow("people",frame)</p>
<p>time.sleep(0.010)</p>
<p># If you press q, exit the loop and close the thread</p>
<p>if cv2.waitKey(10) &amp; 0xFF == ord('q'):</p>
<p>image.release()</p>
<p>break</p>
<p>Camera_display()</p></td>
</tr>
</tbody>
</table>

2.  ### Run the code and test result
    
    cd /home/pi/RaspberryPi-Car/Ai\_recognition/Ai5\_face\_detection
    
    sudo python3 Ai5\_Face\_detection.py
    
    After running the code, the camera will start detecting faces and
    add a green box if a face is recognized.
    
    ![](/media/dabd42af8f6a9e8e0c44d57dfff599b9.png)

<!-- end list -->

## 6.TensorFlow Object Recognition
    
    ![](/media/ebfeea5c5d5f3e2f668edf979f869dfd.png)
    
    TensorFlow is an open source software library that uses data flow
    graphs for numerical calculations. It is one of the widely used
    algorithm libraries that implement machine learning and other
    extensive mathematical operations. TensorFlow is developed by Google
    and is one of the most popular machine learning libraries on GitHub.
    Google uses TensorFlow to implement machine learning in almost all
    applications. For example, if you use Google Photos or Google Voice
    Search, then you indirectly use the TensorFlow model. They work on
    large Google hardware clusters and are powerful in perception tasks.
    
    For more information on
    TensorFlow:<https://www.tensorflow.org/learn>
    
    We only use the object identification model(object\_detection) of
    TensorFlow
    
    Link:
    
    <https://github.com/tensorflow/models/tree/master/research/object_detection>
    
    Link for example code:
    
    <https://github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/object_detection_tutorial.ipynb>
    
    1.  ### Test Code

<table>
<tbody>
<tr class="odd">
<td><p>import numpy as np</p>
<p>import cv2</p>
<p>import os,time</p>
<p>import tensorflow as tf</p>
<p>from object_detection.utils import label_map_util</p>
<p>from object_detection.utils import visualization_utils as vis_utils</p>
<p># Init camera</p>
<p>cap = cv2.VideoCapture(0)</p>
<p>cap.set(3,640) # set Width</p>
<p>cap.set(4,480) # set Height</p>
<p># Init tf model</p>
<p>MODEL_NAME = 'ssdlite_mobilenet_v2_coco' #fast</p>
<p>PATH_TO_CKPT = MODEL_NAME + '/frozen_inference_graph.pb'</p>
<p>PATH_TO_LABELS = os.path.join('data', 'mscoco_label_map.pbtxt')</p>
<p>NUM_CLASSES = 90</p>
<p>IMAGE_SIZE = (12, 8)</p>
<p>fileAlreadyExists = os.path.isfile(PATH_TO_CKPT)</p>
<p>if not fileAlreadyExists:</p>
<p>print('Model does not exsist !')</p>
<p>exit</p>
<p># LOAD GRAPH</p>
<p>print('Loading...')</p>
<p>detection_graph = tf.Graph()</p>
<p>with detection_graph.as_default():</p>
<p>od_graph_def = tf.compat.v1.GraphDef()</p>
<p>with tf.io.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:</p>
<p>serialized_graph = fid.read()</p>
<p>od_graph_def.ParseFromString(serialized_graph)</p>
<p>tf.import_graph_def(od_graph_def, name='')</p>
<p>label_map = label_map_util.load_labelmap(PATH_TO_LABELS)</p>
<p>categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)</p>
<p>category_index = label_map_util.create_category_index(categories)</p>
<p>print('Finish Load Graph..')</p>
<p># Main</p>
<p>t_start = time.time()</p>
<p>fps = 0</p>
<p>with detection_graph.as_default():</p>
<p>with tf.compat.v1.Session(graph=detection_graph) as sess:</p>
<p>while True:</p>
<p>ret, frame = cap.read()</p>
<p># frame = cv2.flip(frame, -1) # Flip camera vertically</p>
<p># frame = cv2.resize(frame,(320,240))</p>
<p>##############</p>
<p>image_np_expanded = np.expand_dims(frame, axis=0)</p>
<p>image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')</p>
<p>detection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')</p>
<p>detection_scores = detection_graph.get_tensor_by_name('detection_scores:0')</p>
<p>detection_classes = detection_graph.get_tensor_by_name('detection_classes:0')</p>
<p>num_detections = detection_graph.get_tensor_by_name('num_detections:0')</p>
<p>print('Running detection..')</p>
<p>(boxes, scores, classes, num) = sess.run(</p>
<p>[detection_boxes, detection_scores, detection_classes, num_detections],</p>
<p>feed_dict=)</p>
<p>print('Done. Visualizing..')</p>
<p>vis_utils.visualize_boxes_and_labels_on_image_array(</p>
<p>frame,</p>
<p>np.squeeze(boxes),</p>
<p>np.squeeze(classes).astype(np.int32),</p>
<p>np.squeeze(scores),</p>
<p>category_index,</p>
<p>use_normalized_coordinates=True,</p>
<p>line_thickness=8)</p>
<p>##############</p>
<p>fps = fps + 1</p>
<p>mfps = fps / (time.time() - t_start)</p>
<p>cv2.putText(frame, "FPS " + str(int(mfps)), (10,10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,255), 2)</p>
<p>cv2.imshow('frame', frame)</p>
<p>k = cv2.waitKey(30) &amp; 0xff</p>
<p>if k == 27: # press 'ESC' to quit</p>
<p>break</p>
<p>cap.release()</p>
<p>cv2.destroyAllWindows()</p></td>
</tr>
</tbody>
</table>

2.  ### Run the code and test result
    
    cd
    /home/pi/RaspberryPi-Car/Ai\_recognition/Ai6\_TensorflowObject\_recognition
    
    sudo python3 Ai6\_Object\_recognition.py
    
    After running the code, the identified problems are circled.
    Raspberry Pi's performance is still not good ,with a low frame rate.
    
    ![](/media/1bea58d02c519ebabdff6d056e032f9c.png)


